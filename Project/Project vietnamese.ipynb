{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab044552",
   "metadata": {},
   "source": [
    "#  Mô hình dự đoán cảm xúc của bài đánh giá phim\n",
    "## Sử dụng PyTorch và SageMaker\n",
    "\n",
    "\n",
    "## Kế hoạch\n",
    "\n",
    "1. Tải dữ liệu.\n",
    "2. Chuẩn bị và xử lý dữ liệu.\n",
    "3. Tải dữ liệu lên S3.\n",
    "4. Build và train PyTorch model.\n",
    "5. Test model đã train.\n",
    "6. Triển khai model đã train.\n",
    "7. Sử dụng model đã triển khai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37301c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 21.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.20.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.19.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (4.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (21.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.23.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.4->boto3>=1.14.12->sagemaker==1.72.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.4->boto3>=1.14.12->sagemaker==1.72.0) (1.26.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker==1.72.0) (1.16.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=388346 sha256=e714adcbc4afd0c4aa0e5cae06ac473c031dbe5e93b9fac66468d172ebb478b4\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/58/70/85faf4437568bfaa4c419937569ba1fe54d44c5db42406bbd7\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.68.0\n",
      "    Uninstalling sagemaker-2.68.0:\n",
      "      Successfully uninstalled sagemaker-2.68.0\n",
      "Successfully installed sagemaker-2.60.0 smdebug-rulesconfig-0.1.4\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Make sure that we use SageMaker 1.x\n",
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26821923",
   "metadata": {},
   "source": [
    "## Bước 1: Tải  dữ liệu\n",
    "\n",
    "Sử dụng tập dữ liệu tại link https://github.com/congnghia0609/ntc-scv/tree/master/data. Đây là tập dữ liệu để phân loại cảm xúc (nhị phân)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4c604b",
   "metadata": {},
   "source": [
    "# Bước 2: Chuẩn bị và xử lý dữ liệu\n",
    "\n",
    "## Để bắt đầu, chúng ta sẽ đọc từng bài đánh giá và kết hợp chúng thành một cấu trúc đầu vào duy nhất. Sau đó, chúng tôi sẽ chia tập dữ liệu thành tập training và tập test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a119ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data_vi'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96610b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 15000 pos / 15000 neg, test = 5000 pos / 5000 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630410ee",
   "metadata": {},
   "source": [
    "## Bây giờ sẽ kết hợp và trộn các đánh giá tích cực và tiêu cực với nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aa49ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c16720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 30000, test = 10000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9825c",
   "metadata": {},
   "source": [
    "## Kiểm tra một vài dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dffe512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhân_viên chậm . . lẩu không ngon . . chỉ có bò cuộn kim_chi là tạm ... quán dễ tìm . .\n",
      "nhiều lúc thiếu bếp nướng nên đợi rất lâu . .\n",
      "sẽ không quay lại\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100])\n",
    "print(train_y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac929ea",
   "metadata": {},
   "source": [
    "## Tinh chỉnh dữ liệu cho phù hợp (xóa các thẻ HTML, chỉnh thành chữ thường, chuẩn hóa unicode, chuẩn hóa kiểu gõ dấu, tách các từ tiếng việt, loại bỏ các dấu câu, xóa các stopword, thay các teencode). Rồi lưu kết quả vào bộ nhớ cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e18ecb2",
   "metadata": {},
   "source": [
    "### Xóa HTML code trong dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "623aea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07429f7e",
   "metadata": {},
   "source": [
    "### Chuẩn hóa Unicode tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8002741f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2021.4.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e80156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    " \n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    " \n",
    " \n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    " \n",
    " \n",
    "dicchar = loaddicchar()\n",
    " \n",
    "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3cfce",
   "metadata": {},
   "source": [
    "### Chuấn hóa kiểu gõ dấu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51e4370b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    " \n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    " \n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    " \n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    " \n",
    " \n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    " \n",
    " \n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a10c73fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anh hòa, đang làm.. gì\n"
     ]
    }
   ],
   "source": [
    "print(chuan_hoa_dau_cau_tieng_viet('anh hoà, đang làm.. gì'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82192a9",
   "metadata": {},
   "source": [
    "### Tách từ tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c3902a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: underthesea in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.3.3)\n",
      "Requirement already satisfied: torch>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (1.4.0)\n",
      "Requirement already satisfied: unidecode in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (1.3.2)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (3.6.2)\n",
      "Requirement already satisfied: transformers>=3.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (4.12.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (4.61.1)\n",
      "Requirement already satisfied: seqeval in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (1.2.2)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (5.4.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (1.0.1)\n",
      "Requirement already satisfied: Click>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (8.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (0.24.2)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (0.9.7)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Click>=6.0->underthesea) (4.8.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (0.8)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (0.1.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (1.19.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (21.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=3.5.0->underthesea) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers>=3.5.0->underthesea) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->Click>=6.0->underthesea) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->underthesea) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->underthesea) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->underthesea) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->underthesea) (2.0.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=3.5.0->underthesea) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->underthesea) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->underthesea) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b44f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e325422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chàng trai 9X Quảng_Trị khởi_nghiệp từ nấm sò'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\n",
    "word_tokenize(sentence, format=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead865b",
   "metadata": {},
   "source": [
    "### Xóa các ký tự không cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2143413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r\"[^a-záàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ]\", \" \",document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067276ce",
   "metadata": {},
   "source": [
    "### Loại bỏ các stopword tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "53a7f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách stopword\n",
    "f = open(\"../data_vi/vietnamese-stopwords-dark.txt\", \"r\")\n",
    "stopwords = f.read()\n",
    "stopwords = stopwords.replace('\\n', ' ').split(' ')\n",
    "\n",
    "f = open(\"../data_vi/teen_code.txt\", \"r\")\n",
    "teencodes = {}\n",
    "for t in f.read().split('\\n'):\n",
    "    t = t.split('\\t')\n",
    "    teencodes[t[0]] = t[1]\n",
    " \n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word not in stopwords:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def replace_teencode(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word in teencodes:\n",
    "            words.append(teencodes[word])\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return words\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb0bcc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    text = text_preprocess(review)\n",
    "    words = text.strip().split()\n",
    "    words = remove_stopwords(words)\n",
    "    words = replace_teencode(words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb205d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trà sữa ngon ko ớn , mùi đặc_trưng , thạch mình thấy rất ngonn . Tuy quán đông nhưng phục_vụ rất nhanh và tận_tình . Lần nào có dịp mình đều qua quán uống hihi . Ủng_hộ quánnn\n",
      "\n",
      "['trà', 'sữa', 'ngon', 'không', 'ớn', 'mùi', 'đặc_trưng', 'thạch', 'ngonn', 'quán', 'đông', 'phục_vụ', 'tận_tình', 'dịp', 'quán', 'uống', 'hihi', 'ủng_hộ', 'quánnn']\n"
     ]
    }
   ],
   "source": [
    "id = 6\n",
    "print(train_X[id])\n",
    "print(review_to_words(train_X[id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a1827",
   "metadata": {},
   "source": [
    "Sử dụng `review_to_words` cho từng bài đánh giá trong tập train và tập test. Ngoài ra, nó lưu kết quả vào bộ nhớ cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29f6ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache_vi\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e43e2563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X_p, test_X_p, train_y_p, test_y_p = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca2005e",
   "metadata": {},
   "source": [
    "## Chuyển đổi dữ liệu\n",
    "\n",
    "Chúng ta tạo 1 tập từ điển bằng cách ánh xạ các từ xuất hiện trong các bài đánh giá với các số nguyên (số lần xuất hiện). Ở đây chúng ta cố định kích thước từ là 5000 từ (trong đó có 2 từ là các khoảng trắng và các từ xuất hiện không thường xuyên tức các từ có tần suất xuất hiện thuộc top 4999 trở về sau được gom thành 1 nhóm). Và chúng ta gắn cho 2 loại từ đó là `0` đối với các khoảng trắng và `1` đối với các từ có tần suất ít. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a021e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    for d in data:\n",
    "        for w in d:\n",
    "            word_count[w] = word_count.get(w, 0) + 1\n",
    "    # Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    # sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    # sorted_words = list(dict(sorted(word_count.items(), key=lambda item: item[1], reverse=True)).items())\n",
    "    sorted_words = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "557f8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a631e7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "không\n",
      "quán\n",
      "ngon\n",
      "món\n",
      "đi\n"
     ]
    }
   ],
   "source": [
    "# Use this space to determine the five most frequently appearing words in the training set.\n",
    "for word in list(word_dict.keys())[:5]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e4868",
   "metadata": {},
   "source": [
    "### Lưu `word_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fcf391d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data_vi/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dc0a61c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7916f",
   "metadata": {},
   "source": [
    "### Chuyển đổi các bài đánh giá\n",
    "\n",
    "Đối với mỗi bình luận chúng ta lấy kích thước cố định là `500` từ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f164bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "baab63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cv, train_X_len = convert_and_pad_data(word_dict, train_X_p)\n",
    "test_X_cv, test_X_len = convert_and_pad_data(word_dict, test_X_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854e4e9e",
   "metadata": {},
   "source": [
    "Kiểm tra một vài bài đánh giá sau khi chuyển đổi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bb569bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3   92   58   25  154 2423  793 1440 1350   22    4  271   16   10\n",
      "    4    3   46   49  508    3   35  131   26    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_X_cv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a3e11",
   "metadata": {},
   "source": [
    "## Bước 3: Tải dữ liệu lên S3\n",
    "\n",
    "Tải tập dữ liệu training lên S3 để mã training có thể truy cập vào nó. Hiện tại, lưu nó cục bộ và sẽ tải lên S3 sau này.\n",
    "\n",
    "### Lưu bộ dữ liệu đào tạo đã xử lý cục bộ\n",
    "\n",
    "Mỗi hàng của tập dữ liệu có dạng `label`,`length`, `review[500]` trong đó `review[500]` là một chuỗi các số nguyên `500` đại diện cho các từ trong bài đánh giá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7bbd543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y_p), pd.DataFrame(train_X_len), pd.DataFrame(train_X_cv)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f6fe4",
   "metadata": {},
   "source": [
    "### Upload dữ liệu training\n",
    "\n",
    "Tiếp theo, tải dữ liệu đào tạo lên AWS S3 mặc định của SageMaker để có thể cung cấp quyền truy cập vào nó trong khi đào tạo mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dc78b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn_vi'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f92596db",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470345da",
   "metadata": {},
   "source": [
    "**NOTE:** Ô ở trên tải lên toàn bộ nội dung của thư mục data, bao gồm tệp `word_dict.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8220f",
   "metadata": {},
   "source": [
    "## Bước 4: Build và Train PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b571e39",
   "metadata": {},
   "source": [
    "### Đầu tiên, ta sẽ tải một phần nhỏ dữ liệu train để làm mẫu. (250 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a3d31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c02d93e",
   "metadata": {},
   "source": [
    "### Viết phương thức training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "51b121c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            # the gradients of all optimized variables are cleared\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model.forward(batch_X)\n",
    "            # calculate the batch loss\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            #optimization\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d7fd4f",
   "metadata": {},
   "source": [
    "#### Kiểm tra xem phương thức ở trên có đang hoạt động hay không trên tập train vừa mới tải."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dde84b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6934336662292481\n",
      "Epoch: 2, BCELoss: 0.6835359334945679\n",
      "Epoch: 3, BCELoss: 0.6749452352523804\n",
      "Epoch: 4, BCELoss: 0.6656753063201905\n",
      "Epoch: 5, BCELoss: 0.6546776771545411\n",
      "Epoch: 6, BCELoss: 0.6404685378074646\n",
      "Epoch: 7, BCELoss: 0.6203627109527587\n",
      "Epoch: 8, BCELoss: 0.5889926671981811\n",
      "Epoch: 9, BCELoss: 0.5643034696578979\n",
      "Epoch: 10, BCELoss: 0.5246706545352936\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 10, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc1c38",
   "metadata": {},
   "source": [
    "### Tạo một pytorch container chứa code train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8df28bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    py_version=\"py3\",\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'hidden_dim': 200,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64687d99",
   "metadata": {},
   "source": [
    "Train với toàn bộ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9d531",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30 14:18:12 Starting - Starting the training job...\n",
      "2021-11-30 14:18:14 Starting - Launching requested ML instances......\n",
      "2021-11-30 14:19:30 Starting - Preparing the instances for training.........\n",
      "2021-11-30 14:20:58 Downloading - Downloading input data...\n",
      "2021-11-30 14:21:23 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,387 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,390 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,403 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,408 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,958 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,959 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,959 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,959 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/69/bf/f0f194d3379d3f3347478bd267f754fc68c11cbf2fe302a6ab69447b1417/beautifulsoup4-4.10.0-py3-none-any.whl (97kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/e3/d9f046b5d1c94a3aeab15f1f867aa414f8ee9d196fae6865f1d6a0ee1a0b/pytz-2021.3-py2.py3-none-any.whl (503kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/97/cd/93ad08b2f97ec95da0bd860380ce0ac7481eaccc760356ee11eda369c048/regex-2021.11.10.tar.gz (702kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train, regex\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qnbm4voo/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\n",
      "2021-11-30 14:21:49 Training - Training image download completed. Training in progress.\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/5d/f5/8d8fa6ffd0251556598eb71da0c5c0374bca976273ade005ed\u001b[0m\n",
      "\u001b[34mSuccessfully built train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, pytz, pandas, regex, tqdm, joblib, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.10.0 html5lib-1.1 joblib-0.14.1 nltk-3.6.2 numpy-1.18.5 pandas-0.24.2 pytz-2021.3 regex-2021.11.10 soupsieve-2.1 tqdm-4.62.3 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-11-30 14:22:16,958 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-11-30 14:22:16,973 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-912257863217/sagemaker-pytorch-2021-11-30-14-18-11-940/source/sourcedir.tar.gz\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"TrainingInputMode\": \"File\"\n",
      "        }\n",
      "    },\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_dim\": 200\n",
      "    },\n",
      "    \"log_level\": 20,\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-11-30-14-18-11-940\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"num_cpus\": 4,\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-912257863217/sagemaker-pytorch-2021-11-30-14-18-11-940/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-11-30-14-18-11-940\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-912257863217/sagemaker-pytorch-2021-11-30-14-18-11-940/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch: 1, BCELoss: 0.6116991750264572\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.49772318642018204\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.4407238682447854\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.39411053213022523\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.3692709803581238\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.3446319840722165\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.3310688198623011\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.32004559242119224\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.32268549880738984\u001b[0m\n",
      "\n",
      "2021-11-30 16:29:21 Uploading - Uploading generated training model\u001b[34mEpoch: 10, BCELoss: 0.3012834822727462\u001b[0m\n",
      "\u001b[34m2021-11-30 16:29:21,180 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-11-30 16:29:28 Completed - Training job completed\n",
      "Training seconds: 7710\n",
      "Billable seconds: 7710\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7547d",
   "metadata": {},
   "source": [
    "## Bước 5: Test model đã train\n",
    "\n",
    "Ta sẽ triển khai model 2 lần\n",
    "\n",
    "Lần đầu sẽ test model này bằng cách triển khai nó và lần thứ hai ta sẽ gửi dữ liệu test đến endpoint đã triển khai. Chúng ta làm điều này để đảm bảo rằng model được triển khai đang hoạt động chính xác.\n",
    "\n",
    "## Bước 6 (lần 1): Triển khai model\n",
    "\n",
    "Sau khi đã train model, model nhận đầu vào dạng `review_length, review [500]`, trong đó `review [500]` là một chuỗi các số nguyên `500` mô tả các từ có trong review, được mã hóa bằng cách sử dụng `word_dict`.\n",
    "\n",
    "Tiếp theo ta sẽ triển khai model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "18611c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367927e2",
   "metadata": {},
   "source": [
    "## Step 7 - Sử dụng model\n",
    "Lấy dữ liệu test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "411ca470",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X_cv)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfb84b9",
   "metadata": {},
   "source": [
    "Sau đó test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "610250bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e08935f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X.values)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64e94b",
   "metadata": {},
   "source": [
    "Tính độ chính xác của model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d0f7f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y_p, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc8a85",
   "metadata": {},
   "source": [
    "### Thử cho một sample và test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40fa3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'Quán cơm sườn, gà nướng đặc biệt luôn, nhìn rất ngon thịt nướng bằng tủ quây, miếng thịt nướng màu đẹp trông ngon lạ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6db4eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert test_review into a form usable by the model and save the results in test_data\n",
    "test_review_data, test_review_len = convert_and_pad_data(word_dict, review_to_words(test_review))\n",
    "\n",
    "test_data = np.array(test_review_data)\n",
    "test_data = np.insert(test_data, 0, test_review_len)\n",
    "\n",
    "test_data = test_data[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "24b1d213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.6123212, dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89880914",
   "metadata": {},
   "source": [
    "Giá trị trả về gần bằng `1`, dự đoán đây là một review thích cực"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a5d1b",
   "metadata": {},
   "source": [
    "### Xóa endpoint\n",
    "\n",
    "Xóa endpoint khi không cần sử dụng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8776bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a72d0",
   "metadata": {},
   "source": [
    "# Triển khai model lên ứng dụng web\n",
    "sau khi train model thành công, ta tải model về, giải nén model sẽ được 3 file (model.pth, model_info.pth, word_dict.pkl)\n",
    "\n",
    "Ta sẽ sử dụng máy ảo EC2 để deloy ứng dụng web với flask\n",
    "\n",
    "Ứng dụng web có cấu trúc như sau:\n",
    "\n",
    "<img src=\"struct.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cde84",
   "metadata": {},
   "source": [
    "File model.py\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.word_dict = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        lengths = x[0,:]\n",
    "        reviews = x[1:,:]\n",
    "        embeds = self.embedding(reviews)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dense(lstm_out)\n",
    "        out = out[(lengths - 1).long(), range(len(lengths))]\n",
    "        return self.sig(out.squeeze())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171db0ba",
   "metadata": {},
   "source": [
    "File predict.py\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from model import LSTMClassifier\n",
    "\n",
    "from utils import review_to_words, convert_and_pad\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\n",
    "    print(\"Loading model.\")\n",
    "\n",
    "    # First, load the parameters used to create the model.\n",
    "    model_info = {}\n",
    "    model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "    with open(model_info_path, 'rb') as f:\n",
    "        model_info = torch.load(f)\n",
    "\n",
    "    print(\"model_info: {}\".format(model_info))\n",
    "\n",
    "    # Determine the device and construct the model.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTMClassifier(model_info['embedding_dim'], model_info['hidden_dim'], model_info['vocab_size'])\n",
    "\n",
    "    # Load the store model parameters.\n",
    "    model_path = os.path.join(model_dir, 'model.pth')\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "\n",
    "    # Load the saved word_dict.\n",
    "    word_dict_path = os.path.join(model_dir, 'word_dict.pkl')\n",
    "    with open(word_dict_path, 'rb') as f:\n",
    "        model.word_dict = pickle.load(f)\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    print(\"Done loading model.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def input_fn(serialized_input_data, content_type):\n",
    "    print('Deserializing the input data.')\n",
    "    if content_type == 'text/plain':\n",
    "        data = serialized_input_data.decode('utf-8')\n",
    "        return data\n",
    "    raise Exception('Requested unsupported ContentType in content_type: ' + content_type)\n",
    "\n",
    "\n",
    "def output_fn(prediction_output, accept):\n",
    "    print('Serializing the generated output.')\n",
    "    return str(prediction_output)\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    print('Inferring sentiment of input data.')\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if model.word_dict is None:\n",
    "        raise Exception('Model has not been loaded properly, no word_dict.')\n",
    "    \n",
    "    # Process input_data so that it is ready to be sent to our model.\n",
    "    # You should produce two variables:\n",
    "    # data_X   - A sequence of length 500 which represents the converted review\n",
    "    # data_len - The length of the review\n",
    "\n",
    "    words = review_to_words(input_data)\n",
    "    data_X, data_len = convert_and_pad(model.word_dict, words)\n",
    "\n",
    "    # Using data_X and data_len we construct an appropriate input tensor. Remember\n",
    "    # that our model expects input data of the form 'len, review[500]'.\n",
    "    data_pack = np.hstack((data_len, data_X))\n",
    "    data_pack = data_pack.reshape(1, -1)\n",
    "    \n",
    "    data = torch.from_numpy(data_pack)\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Make sure to put the model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Compute the result of applying the model to the input data. The variable `result` should\n",
    "    # be a numpy array which contains a single integer which is either 1 or 0\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(data)\n",
    "    \n",
    "    if output > 0.5:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "        \n",
    "    return result\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412911d",
   "metadata": {},
   "source": [
    "File utils.py\n",
    "```python\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def remove_html(txt):\n",
    "    return BeautifulSoup(txt, \"html.parser\").get_text()\n",
    "\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r\"[^a-záàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ]\", \" \", document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document\n",
    "\n",
    "\n",
    "# Danh sách stopword\n",
    "f = open(\"data/vietnamese-stopwords-dark.txt\", \"r\", encoding=\"utf8\")\n",
    "stopwords = f.read()\n",
    "stopwords = stopwords.replace('\\n', ' ').split(' ')\n",
    "\n",
    "f = open(\"data/teen_code.txt\", \"r\", encoding=\"utf8\")\n",
    "teencodes = {}\n",
    "for t in f.read().split('\\n'):\n",
    "    t = t.split('\\t')\n",
    "    teencodes[t[0]] = t[1]\n",
    "\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word not in stopwords:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def replace_teencode(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word in teencodes:\n",
    "            words.append(teencodes[word])\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = text_preprocess(review)\n",
    "    words = text.strip().split()\n",
    "    words = remove_stopwords(words)\n",
    "    words = replace_teencode(words)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0  # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1  # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "\n",
    "    working_sentence = [NOWORD] * pad\n",
    "\n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "\n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a9b76",
   "metadata": {},
   "source": [
    "File app.py\n",
    "```python\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template, redirect\n",
    "import pickle\n",
    "import gzip\n",
    "from predict import model_fn\n",
    "from predict import predict_fn\n",
    "from utils import review_to_words\n",
    "from utils import convert_and_pad\n",
    "app = Flask(__name__)\n",
    "model = model_fn('models/vi')\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "    '''\n",
    "    For rendering results on HTML GUI\n",
    "    '''\n",
    "    result =''\n",
    "    review_input = request.form['text'].encode('utf-8')\n",
    "    # input_data_words = review_to_words(review_input)\n",
    "    # data_X, data_len = convert_and_pad(model.word_dict, input_data_words)\n",
    "    print(review_input)\n",
    "    predict = predict_fn(review_input, model)\n",
    "    if predict == 1:\n",
    "        result = 'Tích cực'\n",
    "    else:\n",
    "        result = 'Tiêu cực'\n",
    "\n",
    "    return render_template('index.html', prediction_text='Kết quả: {}'.format(result))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8639347f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "461b72c7",
   "metadata": {},
   "source": [
    "## Bước 6 (lần 2) - Triển khai model lên ứng dụng web\n",
    "\n",
    "Model đang hoạt động tốt, đã đến lúc tạo một số custom inference code để có thể gửi cho model một bài đánh giá chưa được xử lý để nó xác định cảm xúc của bài đánh giá.\n",
    "\n",
    "Code được lưu trữ trong thư mục `serve`. Trong đó có tệp `model.py` sử dụng để xây dựng model, tệp `utils.py` chứa các hàm tiền xử lý `review_to_words` và` convert_and_pad` mà đã được sử dụng trong quá trình tiền xử lý dữ liệu, và tệp `predict.py` sẽ chứa custom inference code. Lưu ý rằng `requirements.txt` sẽ cho SageMaker biết những thư viện Python nào được yêu cầu bởi custom inference code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07386f1b",
   "metadata": {},
   "source": [
    "### Triển khai model\n",
    "\n",
    "Custom inference code đã được viết, tiếp theo sẽ tạo và triển khai mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb3ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
    "\n",
    "model = PyTorchModel(model_data=estimator.model_data,\n",
    "                     role = role,\n",
    "                     framework_version='0.4.0',\n",
    "                     entry_point='predict.py',\n",
    "                     source_dir='serve_vi',\n",
    "                     predictor_cls=StringPredictor)\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6cc846",
   "metadata": {},
   "source": [
    "### Test model\n",
    "\n",
    "Kiểm tra xem mọi thứ có hoạt động hay không. Kiểm tra model bằng cách tải `250` đánh giá tích cực và tiêu cực đầu tiên và gửi chúng đến endpoint, sau đó quan sát kết quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f7ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def test_reviews(data_dir='../data_vi', stop=250):\n",
    "    \n",
    "    results = []\n",
    "    ground = []\n",
    "    \n",
    "    # We make sure to test both positive and negative reviews    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        \n",
    "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
    "        files = glob.glob(path)\n",
    "        \n",
    "        files_read = 0\n",
    "        \n",
    "        print('Starting ', sentiment, ' files')\n",
    "        \n",
    "        # Iterate through the files and send them to the predictor\n",
    "        for f in files:\n",
    "            with open(f) as review:\n",
    "                # First, we store the ground truth (was the review positive or negative)\n",
    "                if sentiment == 'pos':\n",
    "                    ground.append(1)\n",
    "                else:\n",
    "                    ground.append(0)\n",
    "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
    "                review_input = review.read().encode('utf-8')\n",
    "                # Send the review to the predictor and store the results\n",
    "                # results.append(float(predictor.predict(review_input)))\n",
    "                results.append(int(predictor.predict(review_input)))\n",
    "                \n",
    "            # Sending reviews to our endpoint one at a time takes a while so we\n",
    "            # only send a small number of reviews\n",
    "            files_read += 1\n",
    "            if files_read == stop:\n",
    "                break\n",
    "            \n",
    "    return ground, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground, results = test_reviews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b82158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ground, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e159db",
   "metadata": {},
   "source": [
    "Thử kiểm tra lại `test_review` đã kiểm tra lúc trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(test_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb1551d",
   "metadata": {},
   "source": [
    "Như vậy endpoint đang hoạt động như mong đợi (kết quả khớp với lần test trước), bây giờ ta có thể thiết lập trang web với nó."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f559bf5",
   "metadata": {},
   "source": [
    "## Bước 7 (lần 2): Sử dụng model cho web app\n",
    "\n",
    "Chúng ta muốn tạo một ứng dụng web truy cập vào model. Cách mọi thứ được thiết lập hiện tại khiến điều đó không thể thực hiện được vì để truy cập endpoint SageMaker, trước tiên ứng dụng sẽ phải xác thực với AWS bằng IAM role bao gồm quyền truy cập vào endpoint SageMaker. Tuy nhiên, có một cách dễ dàng hơn là chỉ cần sử dụng một số dịch vụ AWS bổ sung.\n",
    "\n",
    "<img src=\"Web App Diagram.svg\">\n",
    "\n",
    "Sơ đồ trên giúp ta có một cái nhìn tổng quan về cách các dịch vụ khác nhau sẽ hoạt động cùng nhau. Ở ngoài cùng bên phải là model mà ta đã đào tạo ở trên và được triển khai bằng SageMaker. Ở phía ngoài cùng bên trái là ứng dụng web của chúng tôi thu thập đánh giá phim của người dùng.\n",
    "\n",
    "Ở giữa chúng ta tạo một Lambda function, có thể coi đây là một hàm Python đơn giản có thể được thực thi bất cứ khi nào một sự kiện cụ thể xảy ra. Chúng tôi sẽ cấp quyền cho chức năng này để gửi và nhận dữ liệu từ một endpoint của SageMaker.\n",
    "\n",
    "Cuối cùng, phương thức sẽ sử dụng để thực thi hàm Lambda là một endpoint mới mà chúng ta sẽ tạo bằng cách sử dụng API Gateway. Endpoint này sẽ là một url lắng nghe dữ liệu được gửi đến nó. Khi nó nhận được một số dữ liệu, nó sẽ chuyển dữ liệu đó vào Lambda function và sau đó trả về bất cứ thứ gì mà hàm Lambda trả về. Về cơ bản, nó sẽ hoạt động như một giao diện cho phép ứng dụng web của chúng ta giao tiếp với Lambda function.\n",
    "\n",
    "### Thiết lập Lambda function\n",
    "Điều đầu tiên chúng ta sẽ làm là thiết lập một Lambda funtion. Hàm Lambda này sẽ được thực thi bất cứ khi nào public API có dữ liệu được gửi đến nó. Khi nó được thực thi, nó sẽ nhận dữ liệu, thực hiện bất kỳ loại xử lý nào được yêu cầu, gửi dữ liệu (đánh giá) đến endpoint mà ta đã tạo và sau đó trả về kết quả.\n",
    "\n",
    "#### Bước A: Tạo một IAM Role cho Lambda function\n",
    "Vì chúng ta muốn hàm Lambda gọi một endpoint. Để làm điều này, chúng ta sẽ xây dựng một role mà sau này chúng ta có thể cung cấp cho Lambda function.\n",
    "\n",
    "Sử dụng bảng điều khiển AWS, điều hướng đến trang **IAM** và nhấp vào **Roles**. Sau đó, nhấp vào **Create role**. Chọn **AWS service** và chọn **Lambda** làm dịch vụ sẽ sử dụng, sau đó nhấp vào **Next: Permission**.\n",
    "\n",
    "Trong search box, nhập `sagemaker` và chọn check box bên cạnh **AmazonSageMakerFullAccess** sau đó click **Next: Tags**. Sau đó, click vào **Next: Review**.\n",
    "\n",
    "Cuối cùng, đặt tên cho role này. Ví dụ: `LambdaSageMakerRole`. Sau đó, nhấp vào **Create role**.\n",
    "\n",
    "#### Bước B: Tạo một Lambda function\n",
    "Sử dụng bảng điều khiển AWS, điều hướng đến trang AWS Lambda và nhấp vào **Create a function**. Khi đến trang tiếp theo, hãy đảm bảo rằng **Author from scratch** được chọn. Tiếp theo thì đặt tên cho hàm Lambda, ví dụ như `feel_analysis_func`. Đảm bảo rằng **Python 3.6** runtime được chọn và sau đó chọn role đã tạo trong phần trước. Sau đó, nhấp vào **Create Function**.\n",
    "\n",
    "Trên trang tiếp theo, ta sẽ thấy một số thông tin về hàm Lambda mà ta vừa tạo. Nếu cuộn xuống ta sẽ thấy một trình soạn thảo, trong đó ta sẽ viết code sẽ được thực thi khi chức năng Lambda được kích hoạt. Chúng ta sẽ dùng đoạn mã sau:\n",
    "\n",
    "```python\n",
    "# We need to use the low-level library to interact with SageMaker since the SageMaker API\n",
    "# is not available natively through Lambda.\n",
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    # The SageMaker runtime is what allows us to invoke the endpoint that we've created.\n",
    "    runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "    # Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given\n",
    "    response = runtime.invoke_endpoint(EndpointName = '**ENDPOINT NAME HERE**',    # The name of the endpoint we created\n",
    "                                       ContentType = 'text/plain',                 # The data format that is expected\n",
    "                                       Body = event['body'])                       # The actual review\n",
    "\n",
    "    # The response is an HTTP response whose body contains the result of our inference\n",
    "    result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    return {\n",
    "        'statusCode' : 200,\n",
    "        'headers' : { 'Content-Type' : 'text/plain', 'Access-Control-Allow-Origin' : '*' },\n",
    "        'body' : result\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "Khi đã sao chép và dán đoạn mã trên vào trình chỉnh sửa code Lambda, hãy thay thế phần `**ENDPOINT NAME HERE**' bằng tên của endpoint mà chúng ta đã triển khai trước đó. Có thể xác định tên của endpoint bằng cách chạy code bên dưới.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070b96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacd173a",
   "metadata": {},
   "source": [
    "Khi đã thêm tên endpoint vào hàm Lambda, hãy nhấn vào **Save**. Lambda function hiện đã hoạt động. Tiếp theo, chúng ta cần tạo một cách để ứng dụng web của chúng ta thực thi Lambda function.\n",
    "\n",
    "### Thiết lập API Gateway\n",
    "Bây giờ Lambda function của chúng ta đã được thiết lập, tiếp theo ta sẽ tạo một API mới bằng cách sử dụng API Gateway sẽ kích hoạt hàm Lambda mà chúng ta vừa tạo.\n",
    "\n",
    "Sử dụng bảng điều khiển AWS, điều hướng đến **Amazon API Gateway** và sau đó nhấp vào **Get started**.\n",
    "\n",
    "Trên trang tiếp theo, chọn **New API** và đặt tên cho api mới, ví dụ: `sentiment_analysis_api`. Sau đó, nhấp vào **Create API**.\n",
    "\n",
    "Bây giờ chúng ta đã tạo một API, tuy nhiên nó hiện nó không có tác dụng gì. Những gì ta muốn nó làm là kích hoạt Lambda function mà chúng ta đã tạo trước đó.\n",
    "\n",
    "Chọn dropdown menu **Action** và nhấp vào **Create Method**. Một phương thức trống mới sẽ được tạo, hãy chọn dropdown menu của nó và chọn **POST**, sau đó click vào dấu check bên cạnh nó.\n",
    "\n",
    "Đối với integration point, hãy đảm bảo rằng **Lambda Function** được chọn và click vào **Use Lambda Proxy integration**. Tùy chọn này đảm bảo rằng dữ liệu được gửi đến API sau đó sẽ được gửi trực tiếp đến hàm Lambda mà không cần xử lý. Điều đó cũng có nghĩa là giá trị trả về phải là một response object thích hợp vì nó cũng sẽ không được xử lý bởi API Gateway.\n",
    "\n",
    "Nhập tên của Lambda function mà ta đã tạo trước đó vào text box **Lambda Function** và sau đó click vào **Save**. Click vào **OK** trong pop-up box xuất hiện, cấp quyền cho API Gateway để gọi Lambda function mà ta đã tạo.\n",
    "\n",
    "Bước cuối cùng trong việc tạo API Gateway là chọn dropdown menu **Actions** và nhấp vào **Deploy API**. Sau đó đặt tên cho nó ví dụ như `prod`.\n",
    "\n",
    "Bây giờ ta đã thiết lập thành công một public API để truy cập vào model của mình. Copy URL được cung cấp để gọi public API mới tạo vì điều này sẽ cần thiết trong bước tiếp theo. Ta có thể tìm thấy URL này ở đầu trang, được đánh dấu bằng màu xanh lam bên cạnh dòng chữ **Invoke URL**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851551e",
   "metadata": {},
   "source": [
    "## Bước cuối: Triển khai ứng dụng web\n",
    "\n",
    "Bây giờ ta đã có một public API, chúng ta có thể bắt đầu sử dụng nó trong một ứng dụng web.\n",
    "\n",
    "Trong thư mục `website` có một tệp có tên là `index.html`. Tải tệp xuống máy tính và mở tệp đó trong trình soạn thảo văn bản nào đó. Tìm dòng chữ ****REPLACE WITH PUBLIC API URL****. Thay thế chuỗi này bằng url mà ta đã có được.\n",
    "\n",
    "Now, if you open `index.html` on your local computer, your browser will behave as a local web server and you can use the provided site to interact with your SageMaker model.\n",
    "\n",
    "Bây giờ, nếu mở `index.html` trên máy tính của chúng ta, trình duyệt sẽ hoạt động như một máy chủ web cục bộ và ta có thể sử dụng trang web để tương tác với SageMaker Model của mình.\n",
    "\n",
    "> ** Lưu ý quan trọng ** Để ứng dụng web giao tiếp với endpoint, endpoint phải thực sự được triển khai và chạy. Điều này có nghĩa là ta đang trả tiền cho nó. Đảm bảo rằng endpoint đang chạy khi ta muốn sử dụng ứng dụng web nhưng ta nên tắt nó khi không cần đến, nếu không sẽ nhận được một hóa đơn rất lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f5666",
   "metadata": {},
   "source": [
    "Bây giờ ứng dụng web đang hoạt động và đây là kết quả:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f63732",
   "metadata": {},
   "source": [
    "**Ảnh chụp màn hình:**\n",
    "\n",
    "\n",
    "\n",
    "**Kết quả:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82268231",
   "metadata": {},
   "source": [
    "### Xóa endpoint\n",
    "Tắt endpoint khi không còn sử dụng nó nữa. Nếu không sẽ tốn rất nhiều tiền."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a85eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

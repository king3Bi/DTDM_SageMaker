{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c73c7f",
   "metadata": {},
   "source": [
    "#  Mô hình dự đoán cảm xúc qua bài nhận xét\n",
    "## Sử dụng PyTorch và SageMaker\n",
    "\n",
    "\n",
    "## Kế hoạch\n",
    "\n",
    "1. Tải dữ liệu.\n",
    "2. Chuẩn bị và xử lý dữ liệu.\n",
    "3. Tải dữ liệu lên S3.\n",
    "4. Build và train PyTorch model.\n",
    "5. Test model đã train.\n",
    "6. Triển khai model đã train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd2ca475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 21.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.20.4)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.19.1)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Collecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (4.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (21.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.23.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.4->boto3>=1.14.12->sagemaker==1.72.0) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.4->boto3>=1.14.12->sagemaker==1.72.0) (1.26.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker==1.72.0) (1.16.0)\n",
      "Building wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-1.72.0-py2.py3-none-any.whl size=388346 sha256=e714adcbc4afd0c4aa0e5cae06ac473c031dbe5e93b9fac66468d172ebb478b4\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c3/58/70/85faf4437568bfaa4c419937569ba1fe54d44c5db42406bbd7\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: smdebug-rulesconfig, sagemaker\n",
      "  Attempting uninstall: smdebug-rulesconfig\n",
      "    Found existing installation: smdebug-rulesconfig 1.0.1\n",
      "    Uninstalling smdebug-rulesconfig-1.0.1:\n",
      "      Successfully uninstalled smdebug-rulesconfig-1.0.1\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.68.0\n",
      "    Uninstalling sagemaker-2.68.0:\n",
      "      Successfully uninstalled sagemaker-2.68.0\n",
      "Successfully installed sagemaker-2.60.0 smdebug-rulesconfig-0.1.4\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e3eda",
   "metadata": {},
   "source": [
    "## Bước 1: Tải  dữ liệu\n",
    "\n",
    "Sử dụng tập dữ liệu tại link https://github.com/congnghia0609/ntc-scv/tree/master/data. Đây là tập dữ liệu để phân loại cảm xúc (nhị phân)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a002abe",
   "metadata": {},
   "source": [
    "# Bước 2: Chuẩn bị và xử lý dữ liệu\n",
    "\n",
    "## Để bắt đầu, chúng ta sẽ đọc từng bài đánh giá và kết hợp chúng thành một cấu trúc đầu vào duy nhất. Sau đó, chúng tôi sẽ chia tập dữ liệu thành tập training và tập test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da59c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data_vi'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c75bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB reviews: train = 15000 pos / 15000 neg, test = 5000 pos / 5000 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25235682",
   "metadata": {},
   "source": [
    "## Bây giờ sẽ kết hợp và trộn các đánh giá tích cực và tiêu cực với nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379ed90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9c1c6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 30000, test = 10000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5cc22e",
   "metadata": {},
   "source": [
    "## Kiểm tra một vài dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2e7f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhân_viên chậm . . lẩu không ngon . . chỉ có bò cuộn kim_chi là tạm ... quán dễ tìm . .\n",
      "nhiều lúc thiếu bếp nướng nên đợi rất lâu . .\n",
      "sẽ không quay lại\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_X[100])\n",
    "print(train_y[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad8cc6",
   "metadata": {},
   "source": [
    "## Tinh chỉnh dữ liệu cho phù hợp (xóa các thẻ HTML, chỉnh thành chữ thường, chuẩn hóa unicode, chuẩn hóa kiểu gõ dấu, tách các từ tiếng việt, loại bỏ các dấu câu, xóa các stopword, thay các teencode). Rồi lưu kết quả vào bộ nhớ cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f15bc",
   "metadata": {},
   "source": [
    "### Xóa HTML code trong dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "511c4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(txt):\n",
    "    return re.sub(r'<[^>]*>', '', txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb34d02",
   "metadata": {},
   "source": [
    "### Chuẩn hóa Unicode tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8c6ebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (2021.4.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91fff2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    " \n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    " \n",
    " \n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    " \n",
    " \n",
    "dicchar = loaddicchar()\n",
    " \n",
    "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d361c",
   "metadata": {},
   "source": [
    "### Chuấn hóa kiểu gõ dấu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f53511c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    " \n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    " \n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    " \n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    " \n",
    " \n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    " \n",
    " \n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2628161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anh hòa, đang làm.. gì\n"
     ]
    }
   ],
   "source": [
    "print(chuan_hoa_dau_cau_tieng_viet('anh hoà, đang làm.. gì'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa5e86",
   "metadata": {},
   "source": [
    "### Tách từ tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fdce11d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: underthesea in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.3.3)\n",
      "Requirement already satisfied: torch>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (1.4.0)\n",
      "Requirement already satisfied: unidecode in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (1.3.2)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (3.6.2)\n",
      "Requirement already satisfied: transformers>=3.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (4.12.5)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (2.26.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (4.61.1)\n",
      "Requirement already satisfied: seqeval in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (1.2.2)\n",
      "Requirement already satisfied: PyYAML in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (5.4.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (1.0.1)\n",
      "Requirement already satisfied: Click>=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (8.0.1)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (0.24.2)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from underthesea) (0.9.7)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from Click>=6.0->underthesea) (4.8.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (0.8)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (0.1.2)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (1.19.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=3.5.0->underthesea) (21.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=3.5.0->underthesea) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers>=3.5.0->underthesea) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->Click>=6.0->underthesea) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->underthesea) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->underthesea) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->underthesea) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->underthesea) (2.0.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=3.5.0->underthesea) (1.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->underthesea) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->underthesea) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5315e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d0690f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chàng trai 9X Quảng_Trị khởi_nghiệp từ nấm sò'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\n",
    "word_tokenize(sentence, format=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce545b22",
   "metadata": {},
   "source": [
    "### Xóa các ký tự không cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ec22990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r\"[^a-záàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ]\", \" \",document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac515303",
   "metadata": {},
   "source": [
    "### Loại bỏ các stopword tiếng Việt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d846ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh sách stopword\n",
    "f = open(\"../data_vi/vietnamese-stopwords-dark.txt\", \"r\")\n",
    "stopwords = f.read()\n",
    "stopwords = stopwords.replace('\\n', ' ').split(' ')\n",
    "\n",
    "f = open(\"../data_vi/teen_code.txt\", \"r\")\n",
    "teencodes = {}\n",
    "for t in f.read().split('\\n'):\n",
    "    t = t.split('\\t')\n",
    "    teencodes[t[0]] = t[1]\n",
    " \n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word not in stopwords:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "def replace_teencode(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word in teencodes:\n",
    "            words.append(teencodes[word])\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return words\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44389eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_words(review):\n",
    "    text = text_preprocess(review)\n",
    "    words = text.strip().split()\n",
    "    words = remove_stopwords(words)\n",
    "    words = replace_teencode(words)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b04be13d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trà sữa ngon ko ớn , mùi đặc_trưng , thạch mình thấy rất ngonn . Tuy quán đông nhưng phục_vụ rất nhanh và tận_tình . Lần nào có dịp mình đều qua quán uống hihi . Ủng_hộ quánnn\n",
      "\n",
      "['trà', 'sữa', 'ngon', 'không', 'ớn', 'mùi', 'đặc_trưng', 'thạch', 'ngonn', 'quán', 'đông', 'phục_vụ', 'tận_tình', 'dịp', 'quán', 'uống', 'hihi', 'ủng_hộ', 'quánnn']\n"
     ]
    }
   ],
   "source": [
    "id = 6\n",
    "print(train_X[id])\n",
    "print(review_to_words(train_X[id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487cbdc0",
   "metadata": {},
   "source": [
    "Sử dụng `review_to_words` cho từng bài đánh giá trong tập train và tập test. Ngoài ra, nó lưu kết quả vào bộ nhớ cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "42e1b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache_vi\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d2b2f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X_p, test_X_p, train_y_p, test_y_p = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a314a77",
   "metadata": {},
   "source": [
    "## Chuyển đổi dữ liệu\n",
    "\n",
    "Chúng ta tạo 1 tập từ điển bằng cách ánh xạ các từ xuất hiện trong các bài đánh giá với các số nguyên (số lần xuất hiện). Ở đây chúng ta cố định kích thước từ là 5000 từ (trong đó có 2 từ là các khoảng trắng và các từ xuất hiện không thường xuyên tức các từ có tần suất xuất hiện thuộc top 4999 trở về sau được gom thành 1 nhóm). Và chúng ta gắn cho 2 loại từ đó là `0` đối với các khoảng trắng và `1` đối với các từ có tần suất ít. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc15b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_dict(data, vocab_size = 5000):\n",
    "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
    "    \n",
    "    word_count = {} # A dict storing the words that appear in the reviews along with how often they occur\n",
    "    for d in data:\n",
    "        for w in d:\n",
    "            word_count[w] = word_count.get(w, 0) + 1\n",
    "    # Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
    "    # sorted_words[-1] is the least frequently appearing word.\n",
    "    \n",
    "    # sorted_words = list(dict(sorted(word_count.items(), key=lambda item: item[1], reverse=True)).items())\n",
    "    sorted_words = sorted(word_count, key=word_count.get, reverse=True)\n",
    "    \n",
    "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
    "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
    "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
    "        \n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d4e72544",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = build_dict(train_X_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd01c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "không\n",
      "quán\n",
      "ngon\n",
      "món\n",
      "đi\n"
     ]
    }
   ],
   "source": [
    "# Use this space to determine the five most frequently appearing words in the training set.\n",
    "for word in list(word_dict.keys())[:5]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6bf79",
   "metadata": {},
   "source": [
    "### Lưu `word_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fdecc4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data_vi/pytorch' # The folder we will use for storing data\n",
    "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "32b4f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e724c0",
   "metadata": {},
   "source": [
    "### Chuyển đổi các bài đánh giá\n",
    "\n",
    "Đối với mỗi bình luận chúng ta lấy kích thước cố định là `500` từ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ce6b1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "    \n",
    "    working_sentence = [NOWORD] * pad\n",
    "    \n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "            \n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "\n",
    "def convert_and_pad_data(word_dict, data, pad=500):\n",
    "    result = []\n",
    "    lengths = []\n",
    "    \n",
    "    for sentence in data:\n",
    "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
    "        result.append(converted)\n",
    "        lengths.append(leng)\n",
    "        \n",
    "    return np.array(result), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8b9a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_cv, train_X_len = convert_and_pad_data(word_dict, train_X_p)\n",
    "test_X_cv, test_X_len = convert_and_pad_data(word_dict, test_X_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60c3c2",
   "metadata": {},
   "source": [
    "Kiểm tra một vài bài đánh giá sau khi chuyển đổi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27968f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   3   92   58   25  154 2423  793 1440 1350   22    4  271   16   10\n",
      "    4    3   46   49  508    3   35  131   26    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_X_cv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8664d24",
   "metadata": {},
   "source": [
    "## Bước 3: Tải dữ liệu lên S3\n",
    "\n",
    "Tải tập dữ liệu training lên S3 để mã training có thể truy cập vào nó. Hiện tại, lưu nó cục bộ và sẽ tải lên S3 sau này.\n",
    "\n",
    "### Lưu bộ dữ liệu đào tạo đã xử lý cục bộ\n",
    "\n",
    "Mỗi hàng của tập dữ liệu có dạng `label`,`length`, `review[500]` trong đó `review[500]` là một chuỗi các số nguyên `500` đại diện cho các từ trong bài đánh giá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6d65cbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "    \n",
    "pd.concat([pd.DataFrame(train_y_p), pd.DataFrame(train_X_len), pd.DataFrame(train_X_cv)], axis=1) \\\n",
    "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fae44d",
   "metadata": {},
   "source": [
    "### Upload dữ liệu training\n",
    "\n",
    "Tiếp theo, tải dữ liệu đào tạo lên AWS S3 mặc định của SageMaker để có thể cung cấp quyền truy cập vào nó trong khi đào tạo mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cbdddfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/sentiment_rnn_vi'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1eaf381",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00417f27",
   "metadata": {},
   "source": [
    "**NOTE:** Ô ở trên tải lên toàn bộ nội dung của thư mục data, bao gồm tệp `word_dict.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7521a4",
   "metadata": {},
   "source": [
    "## Bước 4: Build và Train PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc7d0b",
   "metadata": {},
   "source": [
    "### Đầu tiên, ta sẽ tải một phần nhỏ dữ liệu train để làm mẫu. (250 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ab908556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# Read in only the first 250 rows\n",
    "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
    "\n",
    "# Turn the input pandas dataframe into tensors\n",
    "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
    "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
    "\n",
    "# Build the dataset\n",
    "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
    "# Build the dataloader\n",
    "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da0e81f",
   "metadata": {},
   "source": [
    "### Viết phương thức training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92169af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:         \n",
    "            batch_X, batch_y = batch\n",
    "            \n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # TODO: Complete this train method to train the model provided.\n",
    "            # the gradients of all optimized variables are cleared\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model.forward(batch_X)\n",
    "            # calculate the batch loss\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            # backpropagation\n",
    "            loss.backward()\n",
    "            #optimization\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data.item()\n",
    "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d289334",
   "metadata": {},
   "source": [
    "#### Kiểm tra xem phương thức ở trên có đang hoạt động hay không trên tập train vừa mới tải."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6e7e626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, BCELoss: 0.6934336662292481\n",
      "Epoch: 2, BCELoss: 0.6835359334945679\n",
      "Epoch: 3, BCELoss: 0.6749452352523804\n",
      "Epoch: 4, BCELoss: 0.6656753063201905\n",
      "Epoch: 5, BCELoss: 0.6546776771545411\n",
      "Epoch: 6, BCELoss: 0.6404685378074646\n",
      "Epoch: 7, BCELoss: 0.6203627109527587\n",
      "Epoch: 8, BCELoss: 0.5889926671981811\n",
      "Epoch: 9, BCELoss: 0.5643034696578979\n",
      "Epoch: 10, BCELoss: 0.5246706545352936\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from train.model import LSTMClassifier\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMClassifier(32, 100, 5000).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "train(model, train_sample_dl, 10, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592ad23",
   "metadata": {},
   "source": [
    "### Tạo một pytorch container chứa code train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4436c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point=\"train.py\",\n",
    "                    source_dir=\"train\",\n",
    "                    role=role,\n",
    "                    framework_version='0.4.0',\n",
    "                    py_version=\"py3\",\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m4.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 10,\n",
    "                        'hidden_dim': 200,\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298460fd",
   "metadata": {},
   "source": [
    "Train với toàn bộ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ce454",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30 14:18:12 Starting - Starting the training job...\n",
      "2021-11-30 14:18:14 Starting - Launching requested ML instances......\n",
      "2021-11-30 14:19:30 Starting - Preparing the instances for training.........\n",
      "2021-11-30 14:20:58 Downloading - Downloading input data...\n",
      "2021-11-30 14:21:23 Training - Downloading the training image..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,387 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,390 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,403 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,408 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,958 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,959 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,959 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2021-11-30 14:21:51,959 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
      "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\u001b[0m\n",
      "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/69/bf/f0f194d3379d3f3347478bd267f754fc68c11cbf2fe302a6ab69447b1417/beautifulsoup4-4.10.0-py3-none-any.whl (97kB)\u001b[0m\n",
      "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
      "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/d3/e3/d9f046b5d1c94a3aeab15f1f867aa414f8ee9d196fae6865f1d6a0ee1a0b/pytz-2021.3-py2.py3-none-any.whl (503kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
      "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/97/cd/93ad08b2f97ec95da0bd860380ce0ac7481eaccc760356ee11eda369c048/regex-2021.11.10.tar.gz (702kB)\u001b[0m\n",
      "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34m  Downloading https://files.pythonhosted.org/packages/63/f3/b7a1b8e40fd1bd049a34566eb353527bb9b8e9b98f8b6cf803bb64d8ce95/tqdm-4.62.3-py2.py3-none-any.whl (76kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
      "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
      "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
      "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
      "  Downloading https://files.pythonhosted.org/packages/02/fb/1c65691a9aeb7bd6ac2aa505b84cb8b49ac29c976411c6ab3659425e045f/soupsieve-2.1-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
      "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train, regex\n",
      "  Running setup.py bdist_wheel for train: started\n",
      "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qnbm4voo/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
      "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
      "\n",
      "2021-11-30 14:21:49 Training - Training image download completed. Training in progress.\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/5d/f5/8d8fa6ffd0251556598eb71da0c5c0374bca976273ade005ed\u001b[0m\n",
      "\u001b[34mSuccessfully built train regex\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy, pytz, pandas, regex, tqdm, joblib, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed beautifulsoup4-4.10.0 html5lib-1.1 joblib-0.14.1 nltk-3.6.2 numpy-1.18.5 pandas-0.24.2 pytz-2021.3 regex-2021.11.10 soupsieve-2.1 tqdm-4.62.3 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
      "\u001b[34mYou are using pip version 18.1, however version 20.3.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2021-11-30 14:22:16,958 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-11-30 14:22:16,973 sagemaker-containers INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-912257863217/sagemaker-pytorch-2021-11-30-14-18-11-940/source/sourcedir.tar.gz\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"RecordWrapperType\": \"None\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"TrainingInputMode\": \"File\"\n",
      "        }\n",
      "    },\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_dim\": 200\n",
      "    },\n",
      "    \"log_level\": 20,\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"job_name\": \"sagemaker-pytorch-2021-11-30-14-18-11-940\",\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"resource_config\": {\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"current_host\": \"algo-1\"\n",
      "    },\n",
      "    \"num_cpus\": 4,\n",
      "    \"user_entry_point\": \"train.py\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-912257863217/sagemaker-pytorch-2021-11-30-14-18-11-940/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2021-11-30-14-18-11-940\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-912257863217/sagemaker-pytorch-2021-11-30-14-18-11-940/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\u001b[0m\n",
      "\u001b[34mUsing device cpu.\u001b[0m\n",
      "\u001b[34mGet train data loader.\u001b[0m\n",
      "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch: 1, BCELoss: 0.6116991750264572\u001b[0m\n",
      "\u001b[34mEpoch: 2, BCELoss: 0.49772318642018204\u001b[0m\n",
      "\u001b[34mEpoch: 3, BCELoss: 0.4407238682447854\u001b[0m\n",
      "\u001b[34mEpoch: 4, BCELoss: 0.39411053213022523\u001b[0m\n",
      "\u001b[34mEpoch: 5, BCELoss: 0.3692709803581238\u001b[0m\n",
      "\u001b[34mEpoch: 6, BCELoss: 0.3446319840722165\u001b[0m\n",
      "\u001b[34mEpoch: 7, BCELoss: 0.3310688198623011\u001b[0m\n",
      "\u001b[34mEpoch: 8, BCELoss: 0.32004559242119224\u001b[0m\n",
      "\u001b[34mEpoch: 9, BCELoss: 0.32268549880738984\u001b[0m\n",
      "\n",
      "2021-11-30 16:29:21 Uploading - Uploading generated training model\u001b[34mEpoch: 10, BCELoss: 0.3012834822727462\u001b[0m\n",
      "\u001b[34m2021-11-30 16:29:21,180 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-11-30 16:29:28 Completed - Training job completed\n",
      "Training seconds: 7710\n",
      "Billable seconds: 7710\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fb5350",
   "metadata": {},
   "source": [
    "## Bước 5: Test model đã train\n",
    "\n",
    "Ta sẽ triển khai model 2 lần\n",
    "\n",
    "Lần đầu sẽ test model này bằng cách triển khai nó và lần thứ hai ta sẽ gửi dữ liệu test đến endpoint đã triển khai. Chúng ta làm điều này để đảm bảo rằng model được triển khai đang hoạt động chính xác.\n",
    "\n",
    "## Bước 6: Triển khai model\n",
    "\n",
    "Sau khi đã train model, model nhận đầu vào dạng `review_length, review [500]`, trong đó `review [500]` là một chuỗi các số nguyên `500` mô tả các từ có trong review, được mã hóa bằng cách sử dụng `word_dict`.\n",
    "\n",
    "Tiếp theo ta sẽ triển khai model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4390781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e0a8c",
   "metadata": {},
   "source": [
    "## Step 7 - Sử dụng model\n",
    "Lấy dữ liệu test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26c40976",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X_cv)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d70c6",
   "metadata": {},
   "source": [
    "Sau đó test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dcf8cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
    "def predict(data, rows=512):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = np.array([])\n",
    "    for array in split_array:\n",
    "        predictions = np.append(predictions, predictor.predict(array))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9d69903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_X.values)\n",
    "predictions = [round(num) for num in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de71df5",
   "metadata": {},
   "source": [
    "Tính độ chính xác của model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6158a272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8461"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y_p, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d4370e",
   "metadata": {},
   "source": [
    "### Thử cho một sample và test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c1dd9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'Quán cơm sườn, gà nướng đặc biệt luôn, nhìn rất ngon thịt nướng bằng tủ quây, miếng thịt nướng màu đẹp trông ngon lạ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb654bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert test_review into a form usable by the model and save the results in test_data\n",
    "test_review_data, test_review_len = convert_and_pad_data(word_dict, review_to_words(test_review))\n",
    "\n",
    "test_data = np.array(test_review_data)\n",
    "test_data = np.insert(test_data, 0, test_review_len)\n",
    "\n",
    "test_data = test_data[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c9d662e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.6123212, dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd157288",
   "metadata": {},
   "source": [
    "Giá trị trả về gần bằng `1`, dự đoán đây là một review thích cực"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42593075",
   "metadata": {},
   "source": [
    "### Xóa endpoint\n",
    "\n",
    "Xóa endpoint khi không cần sử dụng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "23096287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f2889",
   "metadata": {},
   "source": [
    "# Triển khai model lên ứng dụng web\n",
    "sau khi train model thành công, ta tải model về, giải nén model sẽ được 3 file (model.pth, model_info.pth, word_dict.pkl)\n",
    "\n",
    "Ta sẽ sử dụng máy ảo EC2 để deloy ứng dụng web với flask\n",
    "\n",
    "Ứng dụng web có cấu trúc như sau:\n",
    "\n",
    "<img src=\"struct.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d499dc1c",
   "metadata": {},
   "source": [
    "File model.py\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the simple RNN model we will be using to perform Sentiment Analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        \"\"\"\n",
    "        Initialize the model by settingg up the various layers.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.dense = nn.Linear(in_features=hidden_dim, out_features=1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        self.word_dict = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input.\n",
    "        \"\"\"\n",
    "        x = x.t()\n",
    "        lengths = x[0,:]\n",
    "        reviews = x[1:,:]\n",
    "        embeds = self.embedding(reviews)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.dense(lstm_out)\n",
    "        out = out[(lengths - 1).long(), range(len(lengths))]\n",
    "        return self.sig(out.squeeze())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f9041",
   "metadata": {},
   "source": [
    "File predict.py\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from model import LSTMClassifier\n",
    "\n",
    "from utils import review_to_words, convert_and_pad\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\n",
    "    print(\"Loading model.\")\n",
    "\n",
    "    # First, load the parameters used to create the model.\n",
    "    model_info = {}\n",
    "    model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "    with open(model_info_path, 'rb') as f:\n",
    "        model_info = torch.load(f)\n",
    "\n",
    "    print(\"model_info: {}\".format(model_info))\n",
    "\n",
    "    # Determine the device and construct the model.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LSTMClassifier(model_info['embedding_dim'], model_info['hidden_dim'], model_info['vocab_size'])\n",
    "\n",
    "    # Load the store model parameters.\n",
    "    model_path = os.path.join(model_dir, 'model.pth')\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "\n",
    "    # Load the saved word_dict.\n",
    "    word_dict_path = os.path.join(model_dir, 'word_dict.pkl')\n",
    "    with open(word_dict_path, 'rb') as f:\n",
    "        model.word_dict = pickle.load(f)\n",
    "\n",
    "    model.to(device).eval()\n",
    "\n",
    "    print(\"Done loading model.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def input_fn(serialized_input_data, content_type):\n",
    "    print('Deserializing the input data.')\n",
    "    if content_type == 'text/plain':\n",
    "        data = serialized_input_data.decode('utf-8')\n",
    "        return data\n",
    "    raise Exception('Requested unsupported ContentType in content_type: ' + content_type)\n",
    "\n",
    "\n",
    "def output_fn(prediction_output, accept):\n",
    "    print('Serializing the generated output.')\n",
    "    return str(prediction_output)\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    print('Inferring sentiment of input data.')\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if model.word_dict is None:\n",
    "        raise Exception('Model has not been loaded properly, no word_dict.')\n",
    "    \n",
    "    # Process input_data so that it is ready to be sent to our model.\n",
    "    # You should produce two variables:\n",
    "    # data_X   - A sequence of length 500 which represents the converted review\n",
    "    # data_len - The length of the review\n",
    "\n",
    "    words = review_to_words(input_data)\n",
    "    data_X, data_len = convert_and_pad(model.word_dict, words)\n",
    "\n",
    "    # Using data_X and data_len we construct an appropriate input tensor. Remember\n",
    "    # that our model expects input data of the form 'len, review[500]'.\n",
    "    data_pack = np.hstack((data_len, data_X))\n",
    "    data_pack = data_pack.reshape(1, -1)\n",
    "    \n",
    "    data = torch.from_numpy(data_pack)\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Make sure to put the model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Compute the result of applying the model to the input data. The variable `result` should\n",
    "    # be a numpy array which contains a single integer which is either 1 or 0\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(data)\n",
    "    \n",
    "    if output > 0.5:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "        \n",
    "    return result\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c6ca19",
   "metadata": {},
   "source": [
    "File utils.py\n",
    "```python\n",
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def remove_html(txt):\n",
    "    return BeautifulSoup(txt, \"html.parser\").get_text()\n",
    "\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def text_preprocess(document):\n",
    "    # xóa html code\n",
    "    document = remove_html(document)\n",
    "    # chuẩn hóa unicode\n",
    "    document = convert_unicode(document)\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    # tách từ\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    # đưa về lower\n",
    "    document = document.lower()\n",
    "    # xóa các ký tự không cần thiết\n",
    "    document = re.sub(r\"[^a-záàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ]\", \" \", document)\n",
    "    # xóa khoảng trắng thừa\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    return document\n",
    "\n",
    "\n",
    "# Danh sách stopword\n",
    "f = open(\"data/vietnamese-stopwords-dark.txt\", \"r\", encoding=\"utf8\")\n",
    "stopwords = f.read()\n",
    "stopwords = stopwords.replace('\\n', ' ').split(' ')\n",
    "\n",
    "f = open(\"data/teen_code.txt\", \"r\", encoding=\"utf8\")\n",
    "teencodes = {}\n",
    "for t in f.read().split('\\n'):\n",
    "    t = t.split('\\t')\n",
    "    teencodes[t[0]] = t[1]\n",
    "\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word not in stopwords:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def replace_teencode(line):\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word in teencodes:\n",
    "            words.append(teencodes[word])\n",
    "        else:\n",
    "            words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = text_preprocess(review)\n",
    "    words = text.strip().split()\n",
    "    words = remove_stopwords(words)\n",
    "    words = replace_teencode(words)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "def convert_and_pad(word_dict, sentence, pad=500):\n",
    "    NOWORD = 0  # We will use 0 to represent the 'no word' category\n",
    "    INFREQ = 1  # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
    "\n",
    "    working_sentence = [NOWORD] * pad\n",
    "\n",
    "    for word_index, word in enumerate(sentence[:pad]):\n",
    "        if word in word_dict:\n",
    "            working_sentence[word_index] = word_dict[word]\n",
    "        else:\n",
    "            working_sentence[word_index] = INFREQ\n",
    "\n",
    "    return working_sentence, min(len(sentence), pad)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55ec15",
   "metadata": {},
   "source": [
    "File app.py\n",
    "```python\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template, redirect\n",
    "import pickle\n",
    "import gzip\n",
    "from predict import model_fn\n",
    "from predict import predict_fn\n",
    "from utils import review_to_words\n",
    "from utils import convert_and_pad\n",
    "app = Flask(__name__)\n",
    "model = model_fn('models/vi')\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "    '''\n",
    "    For rendering results on HTML GUI\n",
    "    '''\n",
    "    result =''\n",
    "    review_input = request.form['text'].encode('utf-8')\n",
    "    # input_data_words = review_to_words(review_input)\n",
    "    # data_X, data_len = convert_and_pad(model.word_dict, input_data_words)\n",
    "    print(review_input)\n",
    "    predict = predict_fn(review_input, model)\n",
    "    if predict == 1:\n",
    "        result = 'Tích cực'\n",
    "    else:\n",
    "        result = 'Tiêu cực'\n",
    "\n",
    "    return render_template('index.html', prediction_text='Kết quả: {}'.format(result))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='0.0.0.0', port=5000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d5f9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
